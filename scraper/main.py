# Copyright 2024 YÃ¼ksel CoÅŸgun
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import sys

print(">>> Python executable:", sys.executable)
print(">>> sys.path:", sys.path[:3])

# Add the project root to the path for absolute imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scraper.config import load_config
from scraper.core.scraper import Scraper
from scraper.logging_manager.logging_manager import log_message, setup_logging
from scraper.proxy_manager.proxy_rotator import ProxyRotator
from scraper.user_agent_manager.user_agent_manager import UserAgentManager


def main():
    # Loglama ayarlarÄ±nÄ± yapÄ±landÄ±r
    log_file = "logs/scraper.log"
    setup_logging(log_file)
    log_message("INFO", f"Logging initialized. Log file: {log_file}")

    # KullanÄ±cÄ±dan URL alma
    url, output_file = get_user_input()

    # KonfigÃ¼rasyon ayarlarÄ±nÄ± yÃ¼kleme
    config = load_config()

    # UserAgentManager Ã¶rneÄŸi oluÅŸtur ve kullanÄ±cÄ± ajanÄ± seÃ§
    user_agent_manager = UserAgentManager(user_agents=config.get("user_agents", []))
    user_agent = user_agent_manager.get_user_agent()  # Rastgele kullanÄ±cÄ± ajanÄ± seÃ§

    # Proxy rotatÃ¶r oluÅŸtur ve proxy seÃ§
    proxy_rotator = ProxyRotator(proxies=config.get("proxies", []))
    proxy = proxy_rotator.rotate_proxy()  # Rastgele proxy seÃ§

    # KullanÄ±cÄ± ajanÄ± ve proxy'yi konfigÃ¼rasyona ekle
    config["user_agent"] = user_agent
    config["proxy"] = proxy

    # KazÄ±yÄ±cÄ±yÄ± baÅŸlatma
    try:
        scraper = Scraper(url, config)
        print("ğŸš€ Starting enhanced scraping with advanced features...")

        # Fetch data with all advanced features enabled
        result = scraper.fetch_data(
            enable_smart_detection=True, enable_enrichment=True, enable_analysis=True
        )

        print("âœ… Scraping completed with advanced analysis!")

        # Extract the main data for saving
        product_data = result.get("enriched_data", result.get("raw_data", []))

        # EÄŸer veri varsa kaydet
        if product_data:
            scraper.save_data(product_data, output_file)

            # Display summary of advanced features
            print("\nğŸ“Š Advanced Analysis Summary:")
            print(f"   â€¢ Raw data items: {len(result.get('raw_data', []))}")
            print(f"   â€¢ Enriched data items: {len(result.get('enriched_data', []))}")

            if result.get("plugin_results"):
                print(
                    f"   â€¢ Active plugins: {len(result['plugin_results'].get('active_plugins', []))}"
                )
                print(
                    f"   â€¢ Data validation: {len(result['plugin_results'].get('validation_results', []))}"
                )

            if result.get("distributed_results"):
                print(
                    f"   â€¢ Jobs processed: {result['distributed_results'].get('jobs_processed', 0)}"
                )
                print(
                    f"   â€¢ Successful jobs: {result['distributed_results'].get('successful_jobs', 0)}"
                )

            if result.get("price_analysis"):
                print(
                    f"   â€¢ Price outliers detected: {len(result['price_analysis'].outliers)}"
                )
                print(
                    f"   â€¢ Price recommendations: {len(result['price_analysis'].recommendations)}"
                )

            if result.get("comparison_analysis"):
                print(
                    f"   â€¢ Product matches found: {len(result['comparison_analysis'].get('product_matches', []))}"
                )
                print(
                    f"   â€¢ Price comparisons: {len(result['comparison_analysis'].get('price_comparisons', []))}"
                )
                print(
                    f"   â€¢ Deal analyses: {len(result['comparison_analysis'].get('deal_analyses', []))}"
                )

            if result.get("visualization_results"):
                print(f"   â€¢ Charts created: {len(result['visualization_results'])}")
                print(f"   â€¢ Visualizations saved to: data/visualizations/")

            if result.get("reporting_results"):
                print(
                    f"   â€¢ Alerts generated: {len(result['reporting_results'].get('alerts', []))}"
                )
                print(
                    f"   â€¢ Recommendations: {len(result['reporting_results'].get('recommendations', []))}"
                )

            print(f"\nğŸ’¾ Data saved to: {output_file}")
            print("ğŸ“ Additional exports available in 'exports/' directory")
            print("ğŸ“‹ Reports available in 'reports/' directory")
            print("ğŸ“Š Visualizations available in 'data/visualizations/' directory")
            print("ğŸ”Œ Plugin results available in 'plugins/' directory")

            log_message(
                "INFO",
                f"Enhanced scraping completed successfully for URL: {url}. Total products: {len(product_data)}. Output file: {output_file}",
            )
        else:
            log_message("WARNING", f"No products found for URL: {url}.")
            # Create a summary report for debugging
            scraper._create_debug_summary(url)
            print(f"\nğŸ” DEBUGGING INFO:")
            print(f"   â€¢ Check 'debug_html/' directory for saved HTML content")
            print(f"   â€¢ Check 'response.html' for the latest response")
            print(f"   â€¢ Look for '*_analysis.txt' files for content analysis")
            print(f"   â€¢ This will help identify why no products were extracted")

    except KeyboardInterrupt:
        log_message("WARNING", "Scraping interrupted by user")
        print("\nâš ï¸  Scraping interrupted by user (Ctrl+C)")
        print("ğŸ”„ You can run the scraper again when ready.")
        sys.exit(0)
    except Exception as e:
        log_message(
            "ERROR", f"An error occurred during scraping for URL: {url}. Error: {e}"
        )
        sys.exit(1)


def get_user_input():
    """KullanÄ±cÄ±dan URL ve Ã§Ä±ktÄ± dosyasÄ± iÃ§in giriÅŸleri almak."""
    url = input("KazÄ±mak istediÄŸiniz URL'yi girin: ")
    validate_url(url)

    output_file = input(
        "Veriyi kaydetmek iÃ§in Ã§Ä±ktÄ± dosyasÄ±nÄ±n yolunu girin (varsayÄ±lan: data/all_listings.csv): "
    )
    if not output_file:
        output_file = "data/all_listings.csv"

    return url, output_file


def validate_url(url):
    """Verilen URL'nin geÃ§erli bir formatta olup olmadÄ±ÄŸÄ±nÄ± kontrol eder."""
    if not url.startswith(("http://", "https://")):
        log_message(
            "ERROR",
            f"Invalid URL: {url}. URL must start with 'http://' or 'https://'.",
        )
        sys.exit(1)


if __name__ == "__main__":
    main()
